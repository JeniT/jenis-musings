<p>As we encourage linked data adoption within the UK public sector, something we run into again and again is that (unsurprisingly) particular domain areas have pre-existing standard ways of thinking about the data that they care about. There are existing models, often with multiple serialisations, such as in XML and a text-based form, that are supported by existing tool chains.</p>

<p>In contrast, if there is existing RDF in that domain area, it&#39;s usually been designed by people who are more interested in the RDF than in the domain area, and is thus generally more focused on the goals of the typical casual data re-user rather than the professionals in the area.</p>

<!--break-->

<p>To give an example, the international statistics community uses <a href="http://sdmx.org">SDMX</a> for representing and exchanging statistics (and a lot more besides; it&#39;s a huge standard). SDMX includes a well-thought through model for statistical datasets and the observations within them, as well as standard concepts for things like gender, age, unit multipliers and so on. By comparison, <a href="http://sw.joanneum.at/scovo/schema.html">SCOVO</a>, the main RDF model for representing statistics, barely scratches the surface in comparison.</p>

<p>This isn&#39;t the only example: the <a href="http://inspire.jrc.ec.europa.eu/">INSPIRE Directive</a> defines how geographic information must be made available. <a href="http://www.gigateway.org.uk/metadata/standards.html">GEMINI</a> defines the kind of geospatial metadata that that community cares about. The <a href="http://openprovenance.org/">Open Provenance Model</a> is the result of many contributors from multiple fields, and again has a number of serialisations.</p>

<p>You could view this as a challenge: experts in their domains already have models and serialisations for the data that they care about; how can we persuade them to adopt an RDF model and serialisations instead?</p>

<p>But that&#39;s totally the wrong question. Linked data doesn&#39;t, can&#39;t and won&#39;t replace existing ways of handling data. But it has got some interesting features that can bring great benefit to people who want to publish their data, namely:</p>

<ul>
<li><strong>web-scale addresses</strong> -- being able to name and refer to things like individual observations in a statistical hypercube, a particular road junction, or the particular process that led to something being created</li>
<li><strong>annotation</strong> -- the ability to record metadata about everything that you can name, which is everything!</li>
<li><strong>distributed publication</strong> -- enabling multiple publishers to control the publication of their data without having to upload it to a central location</li>
<li><strong>links</strong> -- the joining of information to other information, providing more context, supporting more queries and reducing the requirement for duplication</li>
</ul>

<p>The question is really about how to enable people to reap these benefits; the answer, because HTTP-based addressing and typed linkage is usually hard to introduce into existing formats, is usually to publish data using an RDF-based model alongside existing formats. This might be done by generating an RDF-based format (such as RDF/XML or Turtle) as an alternative to the standard XML or HTML, accessible via content negotiation, or by providing a <a href="http://www.w3.org/TR/grddl/">GRDDL</a> transformation that maps an XML format into RDF/XML.</p>

<p>Either way, the underlying model needs to be mapped into RDF. We&#39;re furthest down this road with <a href="http://groups.google.com/group/publishing-statistical-data">statistical data</a>. I wanted to explore here what it might look like for the Open Provenance Model, building on lessons learned from the statistical domain.</p>

<h2>Open Provenance Model</h2>

<p>The Open Provenance Model talks about three main <strong>nodes</strong>:</p>

<ul>
<li><strong>artifacts</strong>, which are the things that are produced or used by processes</li>
<li><strong>processes</strong>, which are actions that are performed using or producing artifacts</li>
<li><strong>agents</strong>, which are the people or systems that perform actions</li>
</ul>

<p>and five kinds of <strong>edges</strong> that can be defined between them:</p>

<ul>
<li>process A <strong>used</strong> artifact B</li>
<li>artifact A <strong>was generated by</strong> process B</li>
<li>process A <strong>was controlled by</strong> agent B</li>
<li>process A <strong>was triggered by</strong> process B</li>
<li>artifact A <strong>was derived from</strong> artifact B</li>
</ul>

<p>Then things start getting more complicated. OPM indicates that each artifact and agent plays a different <strong>role</strong> when it is used by, generated by or controls a process. What&#39;s more, each artifact and agent might be involved in the process at different <strong>times</strong> (though timing information is optional within OPM). And a given provenance graph may contain several <strong>accounts</strong> of how artifacts, processes and agents fit together.</p>

<h2>Existing Mapping to RDF</h2>

<p>The <a href="http://openprovenance.org/model/opm.owl">OWL ontology for OPM</a> for OPM is a very literal mapping of OPM into RDF. Each of the types of nodes is a separate class, and each of the types of edges is a separate class. Thus, it introduces a lot of n-ary relationships. Take a really simple example of an XML file being transformed into HTML using XSLT. With the OPM ontology, the RDF would look something like:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:transformation a opm:Process .
&lt;doc.html&gt; a opm:Artifact .
&lt;doc.xml&gt; a opm:Artifact .
&lt;doc.xsl&gt; a opm:Artifact .
_:processor a opm:Agent .
_:Jeni a opm:Agent .

_:stylesheetLink a opm:Used ;
  opm:effect _:transformation ;
  opm:cause &lt;doc.xml&gt; ;
  opm:role eg:xsltSource .

_:sourceLink a opm:Used ;
  opm:effect _:transformation ;
  opm:cause &lt;doc.xsl&gt; ;
  opm:role eg:xsltStylesheet .

_:resultLink a opm:WasGeneratedBy ;
  opm:effect &lt;doc.html&gt; ;
  opm:cause _:transformation ;
  opm:role eg:xsltResult .

_:processorLink a opm:WasControlledBy ;
  opm:effect _:transformation ;
  opm:cause _:processor ;
  opm:role xslt:processor .

_:userLink a opm:WasControlledBy ;
  opm:effect _:transformation ;
  opm:cause _:Jeni ;
  opm:role xslt:user .

_:derivation a opm:WasDerivedFrom ;
  opm:effect &lt;doc.html&gt; ;
  opm:cause &lt;doc.xml&gt; .

xslt:source a opm:Role ;
  opm:value &quot;source&quot; .

xslt:stylesheet a opm:Role ;
  opm:value &quot;stylesheet&quot; .

xslt:result a opm:Role ;
  opm:value &quot;result&quot; .

xslt:processor a opm:Role ;
  opm:value &quot;processor&quot; .

xslt:user a opm:Role ;
  opm:value &quot;user&quot; .
</code></pre></div>
<p>To give you an idea of what this mapping means, if I wanted to work out who created <code>doc.html</code>, I would have to do a query like:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">SELECT ?who
WHERE {
  ?generatedBy 
    opm:cause &lt;doc.html&gt; ;
    opm:role xslt:result ;
    opm:effect ?transformation .
  ?controlledBy
    opm:effect ?transformation ;
    opm:role xslt:user ;
    opm:cause ?who .
}
</code></pre></div>
<h2>Some Observations</h2>

<p>There are two things that I want to pull out about the RDF mapping described above.</p>

<ul>
<li>it&#39;s incredibly literal; every entity type within the model is mapped onto an RDF class, including the edges, the roles and the accounts (which I didn&#39;t show above)</li>
<li>it doesn&#39;t reuse any existing vocabularies, even when they might help (such as for the &#39;value&#39; of a role, which is really a label)</li>
</ul>

<p>It reminds me of the mapping of object-oriented or relational data models into each other or into XML, which often result in a god awful mess and people swearing that technology X is goddamned ugly. </p>

<p>The fact is that elegant uses of each modelling paradigm -- ones that are easy to understand and efficient to query -- always take advantage of the unique features of that paradigm. For example, good XML vocabularies take advantage of the distinctions between attributes and elements, of nesting and hierarchies, and of the ability to hold mixed content.</p>

<p>It&#39;s the same with RDF. There are four features of RDF that I think good vocabularies will take suitable advantage of:</p>

<ul>
<li>existing vocabularies</li>
<li>inheritance</li>
<li>shortcuts and reasoning</li>
<li>named graphs</li>
</ul>

<p><strong>Reusing existing vocabularies</strong> takes advantage of the ease of bringing together diverse domains within RDF, and it makes data more reusable. For example, an OPM mapping that encourages the reuse of FOAF for people and organisations saves time and effort for the developers of the OPM RDF vocabulary, that they would otherwise have spent modelling the details of agents; and it means that any agents that are described within the description of a piece of provenance are automatically available as agents in the wider FOAF cloud. The same goes for using DOAP to describe software.</p>

<p>By reusing vocabularies, the data isn&#39;t isolated any more, locked within a single context designed for a single use. This is a huge benefit of the linked data approach and it makes sense to leverage it.</p>

<p><strong>Using inheritance</strong> means creating general purpose classes and properties and encouraging other people to use <code>rdfs:subClassOf</code> or <code>rdfs:subPropertyOf</code> to specialise them according to their own requirements. Within OPM, the different roles that artifacts and agents might play in a process is a natural fit with either sub-properties or sub-classes, depending on how the edges in the model are represented. For example, rather than</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:stylesheetLink a opm:Used ;
  opm:effect _:transformation ;
  opm:cause &lt;doc.xsl&gt; ;
  opm:role eg:xsltStylesheet .

xslt:stylesheet a opm:Role ;
  opm:value &quot;stylesheet&quot; .
</code></pre></div>
<p>you could generate data that looked like:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:stylesheetLink a xslt:Stylesheet ;
  opm:effect _:transformation ;
  opm:cause &lt;doc.xsl&gt; .
</code></pre></div>
<p>where <code>xslt:Stylesheet</code> is defined as a subclass of <code>opm:Used</code>.</p>

<p>Inheritance is a basic form of <strong>reasoning</strong>. In the case of the subclass relationship outlined above, the reasoning is that anything that is a <code>xslt:Stylesheet</code> is also a <code>opm:Used</code>, and thus:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:stylesheetLink a xslt:Stylesheet .
</code></pre></div>
<p>implies</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:stylesheetLink a xslt:Used .
</code></pre></div>
<p>Taking the scenario where you&#39;re doing native linked data publishing -- storing data in a triplestore and then publishing it out from there -- you have two choices:</p>

<ul>
<li>you can store just the basic data, and let the application retrieving it carry out whatever reasoning is necessary to derive the information they need; this limits the size of the triplestore, but can place a large burden on people using it -- either they have to be very familiar with the exact choices made in modelling the basic data, or they have to construct complex SPARQL queries that take account of the fact that the data might be modelled in many different ways</li>
<li>you can store not only the basic data but also anything that can be derived from it; this increases the number of triples you have to store, but means that people can query it without having to perform any reasoning themselves</li>
</ul>

<p>The latter is obviously the more user-friendly approach. (And a triplestore could make it easy by understanding and applying schemas, ontologies and rules as data is loaded in.)</p>

<p>To take a more complex example, provenance could be modelled in a much more direct way, such as:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">&lt;doc.html&gt; a opm:Artifact ;
  opm:derivedFrom &lt;doc.xml&gt; ;
  opm:generatedBy [
    xslt:source &lt;doc.xml&gt; ;
    xslt:stylesheet &lt;doc.xsl&gt; ;
    xslt:processor _:processor ;
    xslt:user _:Jeni ;
  ] .
</code></pre></div>
<p>where <code>xslt:source</code> and <code>xslt:stylesheet</code> are sub-properties of a property called <code>opm:used</code>, and <code>xslt:processor</code> and <code>xslt:user</code> are sub-properties of <code>opm:controlledBy</code>. This removes the n-ary properties, which (given the use of inheritance to represent roles) are only actually needed if the model needs to capture the timing of the involvement of particular artifacts or agents within a process, and makes the provenance information much easier to query than before:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">SELECT ?who
WHERE {
  &lt;doc.html&gt; opm:generatedBy ?transformation .
  ?transformation xslt:user ?who .
}
</code></pre></div>
<p>But what if we also want to support the more complex, n-ary-relation-based models? We would need to assert, somehow, a rule that said that the presence of a <code>opm:controlledBy</code> relationship from a process to an agent was equivalent to having a <code>opm:WasControlledBy</code> instance with a <code>opm:cause</code> pointing to the agent and an <code>opm:effect</code> pointing to the process. Combine this with <code>xslt:user</code> being sub-property of <code>opm:controlledBy</code> and you have the statement:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:transformation xslt:user _:Jeni .
</code></pre></div>
<p>implying:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">_:transformation opm:controlledBy _:Jeni .
</code></pre></div>
<p>which in turn implies:</p>
<div class="highlight"><pre><code class="text language-text" data-lang="text">[] a opm:WasControlledBy ;
  opm:effect _:transformation ;
  opm:cause _:Jeni .
</code></pre></div>
<p>The same reasoning could be applied in the opposite direction, of course. Part of the definition of the use of OPM in RDF could be that the presence of a <code>opm:WasControlledBy</code> with a <code>opm:cause</code> pointing to an agent and <code>opm:effect</code> pointing to a process implies a <code>opm:controlledBy</code> link between the <code>opm:effect</code> and the <code>opm:cause</code>. Whichever was used in the initial modelling of the data, the same query could be used to query the data (accepting some loss of precision along the way, but if you&#39;re not interesting in timing information then why should you suffer the cost of querying through n-ary relations?).</p>

<p>The final thing that I mentioned above that mappings from existing models to RDF should take advantage of is <strong>named graphs</strong>. In OPM, the obvious way that named graphs could play a role is in providing support for the different <em>accounts</em> of provenance. Separate named graphs could be used to represent separate accounts, referencing the same artifacts, agents and processes where appropriate. Individually, the graphs can remain simple; together, you have the full power of OPM.</p>

<h2>Conclusions</h2>

<p>Modelling is a complex design activity, and you&#39;re best off avoiding doing it if you can. That means reusing conceptual models that have been built up for a domain as much as possible and reusing existing vocabularies wherever you can. But you can&#39;t and shouldn&#39;t try to avoid doing design when mapping from a conceptual model to a particular modelling paradigm such as a relational, object-oriented, XML or RDF model.</p>

<p>If you&#39;re mapping to RDF, remember to take advantage of what it&#39;s good at such as web-scale addressing and extensibility, and always bear in mind how easy or difficult your data will be to query. There is no point publishing linked data if it is unusable.</p>
